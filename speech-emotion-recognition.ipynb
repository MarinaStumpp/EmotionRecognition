{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# todo: check all imports --> put everything on top and label\n# Basic imports\nimport pandas as pd\nimport numpy as np\nimport os\nimport sys\nimport csv\n\n# librosa is a Python library for analyzing audio and music. It can be used to extract the data from the audio files we will see it later.\n! apt-get update\n! apt-get install -y libsndfile-dev\nimport librosa\nimport librosa.display\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\n\n# to play the audio files\nfrom IPython.display import Audio\n\n#Tensorflow and keras\nimport tensorflow as tf\nfrom tensorflow import keras\n! pip install np_utils\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization, LSTM, Bidirectional\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras import regularizers\n\nimport warnings\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n\n# Py audio analysis\n! pip install pyAudioAnalysis\n! pip install eyed3\nfrom pyAudioAnalysis import audioBasicIO\nfrom pyAudioAnalysis import ShortTermFeatures\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-28T14:15:47.195264Z","iopub.execute_input":"2022-06-28T14:15:47.195895Z","iopub.status.idle":"2022-06-28T14:16:52.179953Z","shell.execute_reply.started":"2022-06-28T14:15:47.1958Z","shell.execute_reply":"2022-06-28T14:16:52.178768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## todo: reference repository\n\n# Paths for data.\ndef preprocess_ravdess_data():\n    ravdess = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/\"\n    ravdess_directory_list = os.listdir(ravdess)\n\n    file_emotion = []\n    file_path = []\n    for dir in ravdess_directory_list:\n        # as their are 20 different actors in our previous directory we need to extract files for each actor.\n        actor = os.listdir(ravdess + dir)\n        for file in actor:\n            part = file.split('.')[0]\n            part = part.split('-')\n            # third part in each file represents the emotion associated to that file.\n            file_emotion.append(int(part[2]))\n            file_path.append(ravdess + dir + '/' + file)\n\n    # dataframe for emotion of files\n    emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n\n    # dataframe for path of files.\n    path_df = pd.DataFrame(file_path, columns=['Path'])\n    data_path = pd.concat([emotion_df, path_df], axis=1)\n\n    # changing integers to actual emotions.\n    data_path.Emotions.replace({1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}, inplace=True)\n    data_path.to_csv(\"data_path.csv\",index=False)\n    data_path.head()\n    \n    return data_path","metadata":{"execution":{"iopub.status.busy":"2022-06-28T14:16:52.182236Z","iopub.execute_input":"2022-06-28T14:16:52.183359Z","iopub.status.idle":"2022-06-28T14:16:52.195072Z","shell.execute_reply.started":"2022-06-28T14:16:52.183318Z","shell.execute_reply":"2022-06-28T14:16:52.194054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_short_term_features(path): # with py audioanalysis\n\n    [Fs, data] = audioBasicIO.read_audio_file(path)# read the wav file\n    data = audioBasicIO.stereo_to_mono(data) \n    results, feature_names = ShortTermFeatures.feature_extraction(data, Fs, 0.050*Fs, 0.025*Fs, deltas=False)\n    if results.shape[1]>100:\n        results=results[:,:100]\n    elif results.shape[1]<100:\n        padding = np.zeros((34,100))\n        padding[:results.shape[0],:results.shape[1]]=results\n        results=padding\n    results = np.transpose(results)\n    return results","metadata":{"execution":{"iopub.status.busy":"2022-06-28T14:16:52.196662Z","iopub.execute_input":"2022-06-28T14:16:52.197734Z","iopub.status.idle":"2022-06-28T14:16:52.209425Z","shell.execute_reply.started":"2022-06-28T14:16:52.197693Z","shell.execute_reply":"2022-06-28T14:16:52.208137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_mfcc_features(path): # with py audioanalysis\n    # ZCR\n    data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)\n    result = np.array([])\n    results=[]\n    #zcr = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)\n    #result=np.hstack((result, zcr)) # stacking horizontally\n\n    # Chroma_stft\n    #stft = np.abs(librosa.stft(data))\n    #chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n    #result = np.hstack((result, chroma_stft)) # stacking horizontally\n\n    # MFCC\n    results = librosa.feature.mfcc(y=data, sr=sample_rate, n_mfcc=13).T\n    \n    if results.shape[0]>100:\n        results=results[:100,:]\n    elif results.shape[0]<100:\n        padding = np.zeros((100,13))\n        padding[:results.shape[0],:results.shape[1]]=results\n        results=padding\n    #result = np.hstack((result, mfcc)) # stacking horizontally\n    #results.append(mfcc)\n    #results=np.stack(results)\n\n    # Root Mean Square Value\n    #rms = np.mean(librosa.feature.rms(y=data).T, axis=0)\n    #result = np.hstack((result, rms)) # stacking horizontally\n\n    # MelSpectogram\n    #mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate).T, axis=0)\n    #result = np.hstack((result, mel)) # stacking horizontally\n    \n    return results","metadata":{"execution":{"iopub.status.busy":"2022-06-28T15:46:45.152765Z","iopub.execute_input":"2022-06-28T15:46:45.153138Z","iopub.status.idle":"2022-06-28T15:46:45.16351Z","shell.execute_reply.started":"2022-06-28T15:46:45.153107Z","shell.execute_reply":"2022-06-28T15:46:45.162103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-06-28T15:46:48.617738Z","iopub.execute_input":"2022-06-28T15:46:48.618664Z","iopub.status.idle":"2022-06-28T15:46:48.734322Z","shell.execute_reply.started":"2022-06-28T15:46:48.618626Z","shell.execute_reply":"2022-06-28T15:46:48.733231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get all features of all files by using py audio\ndef extract_features(data_path, feature_type):\n    features, labels = [], []\n    for path, emotion in zip(data_path.Path, data_path.Emotions):\n        if feature_type == 'all':\n            #34 features\n            extract_short_term_features(path)\n        elif feature_type == 'mfcc':\n            #mfcc\n            feature=extract_mfcc_features(path)\n        else:\n            print(\"Feature type not available\")\n        features.append(feature)\n        labels.append(emotion)\n    np.save('features', features)\n    np.save('labels', labels)\n    \n    return features, labels","metadata":{"execution":{"iopub.status.busy":"2022-06-28T16:10:53.290281Z","iopub.execute_input":"2022-06-28T16:10:53.290651Z","iopub.status.idle":"2022-06-28T16:10:53.298239Z","shell.execute_reply.started":"2022-06-28T16:10:53.290622Z","shell.execute_reply":"2022-06-28T16:10:53.29697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## To read the data from database files\n##todo insert path\ndef load_data_and_features(feature_type):\n    if feature_type == 'all':\n        #34 features\n        features = np.load('../input/features-labels-datapath/features.npy')\n        labels = np.load('../input/features-labels-datapath/labels.npy')\n    elif feature_type == 'mfcc':\n        #mfcc\n        features = np.load('../input/features-labels-mfcc/features_mfcc.npy')\n        labels = np.load('../input/features-labels-mfcc/labels_mfcc.npy')\n    else:\n            print(\"Feature type not available\")\n    data_path = pd.read_csv('../input/features-labels-datapath/data_path.csv')\n    return features, labels, data_path","metadata":{"execution":{"iopub.status.busy":"2022-06-28T16:10:54.065798Z","iopub.execute_input":"2022-06-28T16:10:54.066147Z","iopub.status.idle":"2022-06-28T16:10:54.07288Z","shell.execute_reply.started":"2022-06-28T16:10:54.066117Z","shell.execute_reply":"2022-06-28T16:10:54.071747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from sklearn.feature_selection import VarianceThreshold\n\ndef prepare_training_data(data_available, feature_type):\n    # load data or generate features out of the RAVDESS database\n    if data_available:\n        features,labels, data_path= load_data_and_features(feature_type)\n    else:\n        data_path = preprocess_ravdess_data()\n        features, labels = extract_features(data_path, feature_type)\n    X = np.stack(features)\n    Y = labels\n    #One hot encoding the labels\n    encoder = OneHotEncoder()\n    Y = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()\n    # splitting data\n    x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=0, shuffle=True, test_size=0.1)\n    # scaling data with sklearn's Standard scaler\n    scaler = StandardScaler()\n    x_train = scaler.fit_transform(x_train.reshape(-1, x_train.shape[-1])).reshape(x_train.shape)\n    x_test = scaler.transform(x_test.reshape(-1, x_test.shape[-1])).reshape(x_test.shape)\n \n    return x_train, x_test, y_train, y_test, encoder","metadata":{"execution":{"iopub.status.busy":"2022-06-28T16:11:07.72921Z","iopub.execute_input":"2022-06-28T16:11:07.729583Z","iopub.status.idle":"2022-06-28T16:11:07.739896Z","shell.execute_reply.started":"2022-06-28T16:11:07.729551Z","shell.execute_reply":"2022-06-28T16:11:07.738813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_original_speech_model(input_shape):\n    inputs = keras.Input(shape=(input_shape))\n\n    #Low level features\n    low_level = layers.Bidirectional(LSTM(256, return_sequences=True))(inputs)\n    low_level = layers.Dropout(0.2)(low_level)\n    low_level = layers.Bidirectional(LSTM(256))(low_level)\n    low_level = layers.Dropout(0.2)(low_level)\n    low_level = layers.Dense(8, activation='relu')(low_level)\n    \n    #High level features \n    high_level = layers.Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu')(inputs)\n    high_level = layers.Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu')(high_level)\n    high_level = layers.Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu')(high_level)\n    high_level = layers.Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu')(high_level)\n    high_level = layers.MaxPooling1D(pool_size=2, strides = 2, padding = 'same')(high_level)\n    high_level = layers.Dropout(0.2)(high_level)\n    high_level = layers.Dense(8, activation='relu')(high_level)\n    high_level = layers.Flatten()(high_level)\n\n    #Concatenate low and high level features\n    merge = layers.concatenate([low_level, high_level], axis=1)\n    merge = layers.Flatten()(merge)\n\n    outputs = Dense(8, activation='softmax')(merge)\n\n    model = keras.Model(inputs, outputs)\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-06-28T14:16:52.250393Z","iopub.execute_input":"2022-06-28T14:16:52.251337Z","iopub.status.idle":"2022-06-28T14:16:52.264338Z","shell.execute_reply.started":"2022-06-28T14:16:52.251175Z","shell.execute_reply":"2022-06-28T14:16:52.263137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_LSTM_speech_model(input_shape):\n    inputs = keras.Input(shape=(input_shape))\n\n    #Low level features\n    low_level = layers.Bidirectional(LSTM(256, return_sequences=True))(inputs)\n    low_level = layers.Dropout(0.2)(low_level)\n    low_level = layers.Bidirectional(LSTM(256))(low_level)\n    low_level = layers.Dropout(0.2)(low_level)\n    outputs = layers.Dense(8, activation='softmax')(low_level)\n\n    model = keras.Model(inputs, outputs)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-06-28T14:16:52.266355Z","iopub.execute_input":"2022-06-28T14:16:52.267118Z","iopub.status.idle":"2022-06-28T14:16:52.279189Z","shell.execute_reply.started":"2022-06-28T14:16:52.267078Z","shell.execute_reply":"2022-06-28T14:16:52.278065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_CNN_speech_model(input_shape):\n    inputs = keras.Input(shape=(input_shape))\n    \n    #High level features \n    high_level = layers.Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu')(inputs)\n    high_level = layers.Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu')(high_level)\n    high_level = layers.Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu')(high_level)\n    high_level = layers.Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu')(high_level)\n    high_level = layers.MaxPooling1D(pool_size=2, strides = 2, padding = 'same')(high_level)\n    high_level = layers.Dropout(0.2)(high_level)\n    high_level = layers.Flatten()(high_level)\n\n    outputs = layers.Dense(8, activation='softmax')(high_level)\n\n    model = keras.Model(inputs, outputs)\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-06-28T14:16:52.282084Z","iopub.execute_input":"2022-06-28T14:16:52.283762Z","iopub.status.idle":"2022-06-28T14:16:52.29345Z","shell.execute_reply.started":"2022-06-28T14:16:52.28372Z","shell.execute_reply":"2022-06-28T14:16:52.292464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This model reaches up to 70-75% validation accuracy\ndef create_test_speech_model(input_shape):\n    inputs = keras.Input(shape=(input_shape))\n\n    #Low level feature\n    low_level = layers.Bidirectional(LSTM(64, return_sequences=True))(inputs)\n    low_level = layers.Dropout(0.5)(low_level)\n    low_level = layers.Bidirectional(LSTM(32))(low_level)\n    low_level = layers.Dropout(0.5)(low_level)\n    low_level = layers.Dense(128)(low_level)\n    low_level = layers.Dropout(0.5)(low_level)\n    low_level = layers.Dense(64)(low_level)\n    low_level = layers.Dropout(0.5)(low_level)\n    \n    #High level features \n    high_level = layers.Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu')(inputs)\n    high_level = layers.MaxPooling1D(pool_size=5, strides = 2, padding = 'same')(high_level)\n    high_level = layers.Dropout(0.4)(high_level)\n    \n    high_level = layers.Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu')(high_level)\n    high_level = layers.MaxPooling1D(pool_size=5, strides = 2, padding = 'same')(high_level)\n    high_level = layers.Dropout(0.3)(high_level)\n    \n    high_level = layers.Conv1D(64, kernel_size=3, strides=1, padding='same', activation='relu')(high_level)\n    high_level = layers.MaxPooling1D(pool_size=3, strides = 2, padding = 'same')(high_level)\n    high_level = layers.Dropout(0.3)(high_level)\n    \n    high_level = layers.Conv1D(64, kernel_size=3, strides=1, padding='same', activation='relu')(high_level)\n    high_level = layers.MaxPooling1D(pool_size=3, strides = 2, padding = 'same')(high_level)\n    high_level = layers.Dropout(0.3)(high_level)\n    high_level = layers.Flatten()(high_level)\n\n    #Concatenate low and high level features\n    merge = layers.concatenate([low_level, high_level], axis=1)\n    merge = layers.Flatten()(merge)\n\n    outputs = Dense(8, activation='softmax')(merge)\n\n    model = keras.Model(inputs, outputs)\n    return model\n    ","metadata":{"execution":{"iopub.status.busy":"2022-06-28T16:55:00.262787Z","iopub.execute_input":"2022-06-28T16:55:00.263238Z","iopub.status.idle":"2022-06-28T16:55:00.290932Z","shell.execute_reply.started":"2022-06-28T16:55:00.263199Z","shell.execute_reply":"2022-06-28T16:55:00.289543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This model reaches over 80% validation accuracy\ndef create_CNN_test_speech_model(input_shape):\n    inputs = keras.Input(shape=(input_shape))\n    \n    #High level features \n    high_level = layers.Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu')(inputs)\n    high_level = layers.MaxPooling1D(pool_size=5, strides = 2, padding = 'same')(high_level)\n    high_level = layers.Dropout(0.4)(high_level)\n    \n    high_level = layers.Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu')(high_level)\n    high_level = layers.MaxPooling1D(pool_size=5, strides = 2, padding = 'same')(high_level)\n    high_level = layers.Dropout(0.3)(high_level)\n    \n    high_level = layers.Conv1D(64, kernel_size=3, strides=1, padding='same', activation='relu')(high_level)\n    high_level = layers.MaxPooling1D(pool_size=3, strides = 2, padding = 'same')(high_level)\n    high_level = layers.Dropout(0.3)(high_level)\n    \n    high_level = layers.Conv1D(64, kernel_size=3, strides=1, padding='same', activation='relu')(high_level)\n    high_level = layers.MaxPooling1D(pool_size=3, strides = 2, padding = 'same')(high_level)\n    high_level = layers.Dropout(0.3)(high_level)\n    high_level = layers.Flatten()(high_level)\n\n    outputs = layers.Dense(8, activation='softmax')(high_level)\n\n    model = keras.Model(inputs, outputs)\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-06-28T16:52:49.950834Z","iopub.execute_input":"2022-06-28T16:52:49.951188Z","iopub.status.idle":"2022-06-28T16:52:49.96301Z","shell.execute_reply.started":"2022-06-28T16:52:49.951157Z","shell.execute_reply":"2022-06-28T16:52:49.962044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This model reaches up to 75 % validation accuracy\ndef create_LSTM_test_speech_model(input_shape):\n    inputs = keras.Input(shape=(input_shape))\n\n    low_level = layers.Bidirectional(LSTM(128, return_sequences=True))(inputs)\n    low_level = layers.Dropout(0.5)(low_level)\n    low_level = layers.Bidirectional(LSTM(64))(low_level)\n    low_level = layers.Dropout(0.5)(low_level)\n    low_level = layers.Dense(128)(low_level)\n    low_level = layers.Dropout(0.5)(low_level)\n    low_level = layers.Dense(64)(low_level)\n    low_level = layers.Dropout(0.5)(low_level)\n    \n    outputs = layers.Dense(8, activation='softmax')(low_level)\n\n    model = keras.Model(inputs, outputs)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-06-28T16:50:22.326637Z","iopub.execute_input":"2022-06-28T16:50:22.327257Z","iopub.status.idle":"2022-06-28T16:50:22.337142Z","shell.execute_reply.started":"2022-06-28T16:50:22.327084Z","shell.execute_reply":"2022-06-28T16:50:22.336124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_available = True\nfeature_type='mfcc' #mfcc or all\nepochs=100\nx_train, x_test, y_train, y_test, encoder = prepare_training_data(data_available, feature_type)\nn_classes = len(y_train[0])\nmodel=create_CNN_test_speech_model(x_train.shape[1:])\nopt = keras.optimizers.Adam()\nmodel.compile(optimizer = opt , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\nmodel.summary()\nrlrp = ReduceLROnPlateau(monitor='loss', factor=0.5, verbose=1, patience=5, min_lr=0.0000001)\nearlystopping = EarlyStopping(monitor =\"val_accuracy\", mode = 'auto', patience =10, restore_best_weights = True)\nhistory=model.fit(x_train, y_train, batch_size=64,validation_split=0.15, epochs=epochs, callbacks=[rlrp, earlystopping], shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T16:55:51.406782Z","iopub.execute_input":"2022-06-28T16:55:51.40714Z","iopub.status.idle":"2022-06-28T16:56:03.461884Z","shell.execute_reply.started":"2022-06-28T16:55:51.407109Z","shell.execute_reply":"2022-06-28T16:56:03.460962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Just some vizualizations - nothing final yet - todo\n\nprint(\"Accuracy of our model on test data : \" , model.evaluate(x_test,y_test)[1]*100 , \"%\")\nepochs = [i for i in range(len(history.history['loss']))]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\ntest_acc = history.history['val_accuracy']\ntest_loss = history.history['val_loss']\n\nfig.set_size_inches(20,6)\nax[0].plot(epochs , train_loss , label = 'Training Loss')\nax[0].plot(epochs , test_loss , label = 'Validatoin Loss')\nax[0].set_title('Training & Validation Loss')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\n\nax[1].plot(epochs , train_acc , label = 'Training Accuracy')\nax[1].plot(epochs , test_acc , label = 'Validation Accuracy')\nax[1].set_title('Training & Validation Accuracy')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nplt.savefig('plot.png')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-28T16:56:33.487472Z","iopub.execute_input":"2022-06-28T16:56:33.488359Z","iopub.status.idle":"2022-06-28T16:56:34.001479Z","shell.execute_reply.started":"2022-06-28T16:56:33.488312Z","shell.execute_reply":"2022-06-28T16:56:34.000534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predicting on test data.\npred_test = model.predict(x_test)\ny_pred = encoder.inverse_transform(pred_test)\n\ny_test = encoder.inverse_transform(y_test)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T16:56:37.004171Z","iopub.execute_input":"2022-06-28T16:56:37.004549Z","iopub.status.idle":"2022-06-28T16:56:37.135367Z","shell.execute_reply.started":"2022-06-28T16:56:37.004514Z","shell.execute_reply":"2022-06-28T16:56:37.134344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\ndf['Predicted Labels'] = y_pred.flatten()\ndf['Actual Labels'] = y_test.flatten()\n\ndf.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T16:56:38.079285Z","iopub.execute_input":"2022-06-28T16:56:38.079961Z","iopub.status.idle":"2022-06-28T16:56:38.094019Z","shell.execute_reply.started":"2022-06-28T16:56:38.079924Z","shell.execute_reply":"2022-06-28T16:56:38.092914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred)\ncmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\nplt.figure(figsize = (12, 10))\ncmn = pd.DataFrame(cm , index = [i for i in encoder.categories_] , columns = [i for i in encoder.categories_])\nsns.heatmap(cmn, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')\nplt.title('Confusion Matrix', size=20)\nplt.xlabel('Predicted Labels', size=14)\nplt.ylabel('Actual Labels', size=14)\nplt.savefig('confusion.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-28T16:57:41.717635Z","iopub.execute_input":"2022-06-28T16:57:41.718522Z","iopub.status.idle":"2022-06-28T16:57:42.402463Z","shell.execute_reply.started":"2022-06-28T16:57:41.718451Z","shell.execute_reply":"2022-06-28T16:57:42.401441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred)\ncmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\nplt.figure(figsize = (12, 10))\ncmn = pd.DataFrame(cmn , index = [i for i in encoder.categories_] , columns = [i for i in encoder.categories_])\nsns.heatmap(cmn, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')\nplt.title('Confusion Matrix', size=20)\nplt.xlabel('Predicted Labels', size=14)\nplt.ylabel('Actual Labels', size=14)\nplt.savefig('confusion.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-24T12:19:45.60135Z","iopub.execute_input":"2022-06-24T12:19:45.601791Z","iopub.status.idle":"2022-06-24T12:19:46.346689Z","shell.execute_reply.started":"2022-06-24T12:19:45.601753Z","shell.execute_reply":"2022-06-24T12:19:46.344725Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-28T14:18:37.656201Z","iopub.execute_input":"2022-06-28T14:18:37.657119Z","iopub.status.idle":"2022-06-28T14:18:37.671503Z","shell.execute_reply.started":"2022-06-28T14:18:37.657078Z","shell.execute_reply":"2022-06-28T14:18:37.670417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}