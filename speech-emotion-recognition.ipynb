{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# todo: check all imports --> put everything on top and label\nimport pandas as pd\nimport numpy as np\n\nimport os\nimport sys\n\n# librosa is a Python library for analyzing audio and music. It can be used to extract the data from the audio files we will see it later.\n! apt-get update\n! apt-get install -y libsndfile-dev\nimport librosa\nimport librosa.display\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\n\n# to play the audio files\nfrom IPython.display import Audio\n\nimport tensorflow.keras\n! pip install np_utils\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\nimport warnings\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n\n! pip install pyAudioAnalysis\n! pip install eyed3","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-12T14:42:42.760480Z","iopub.execute_input":"2022-06-12T14:42:42.760982Z","iopub.status.idle":"2022-06-12T14:43:43.021684Z","shell.execute_reply.started":"2022-06-12T14:42:42.760899Z","shell.execute_reply":"2022-06-12T14:43:43.020363Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from keras.layers import Bidirectional\nfrom tensorflow.keras.layers import Dense, LSTM, Flatten, Dropout, BatchNormalization\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nfrom pyAudioAnalysis import audioBasicIO\nfrom pyAudioAnalysis import ShortTermFeatures\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-06-12T15:08:22.453318Z","iopub.execute_input":"2022-06-12T15:08:22.453666Z","iopub.status.idle":"2022-06-12T15:08:22.518080Z","shell.execute_reply.started":"2022-06-12T15:08:22.453638Z","shell.execute_reply":"2022-06-12T15:08:22.517317Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"## todo: reference repository\n\n# Paths for data.\ndef preprocess_ravdess_data():\n    ravdess = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/\"\n    ravdess_directory_list = os.listdir(ravdess)\n\n    file_emotion = []\n    file_path = []\n    for dir in ravdess_directory_list:\n        # as their are 20 different actors in our previous directory we need to extract files for each actor.\n        actor = os.listdir(ravdess + dir)\n        for file in actor:\n            part = file.split('.')[0]\n            part = part.split('-')\n            # third part in each file represents the emotion associated to that file.\n            file_emotion.append(int(part[2]))\n            file_path.append(ravdess + dir + '/' + file)\n\n    # dataframe for emotion of files\n    emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n\n    # dataframe for path of files.\n    path_df = pd.DataFrame(file_path, columns=['Path'])\n    data_path = pd.concat([emotion_df, path_df], axis=1)\n\n    # changing integers to actual emotions.\n    data_path.Emotions.replace({1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}, inplace=True)\n    data_path.to_csv(\"data_path.csv\",index=False)\n    data_path.head()\n    \n    return data_path","metadata":{"execution":{"iopub.status.busy":"2022-06-12T15:03:11.396093Z","iopub.execute_input":"2022-06-12T15:03:11.396880Z","iopub.status.idle":"2022-06-12T15:03:11.407913Z","shell.execute_reply.started":"2022-06-12T15:03:11.396840Z","shell.execute_reply":"2022-06-12T15:03:11.407062Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"def extract__short_term_features(path): # with py audioanalysis\n\n    [Fs, data] = audioBasicIO.read_audio_file(path)# read the wav file\n    data = audioBasicIO.stereo_to_mono(data) \n    results, feature_names = ShortTermFeatures.feature_extraction(data, Fs, 0.050*Fs, 0.025*Fs, deltas=False)\n    if results.shape[1]>100:\n        results=results[:,:100]\n    elif results.shape[1]<100:\n        padding = np.zeros((34,100))\n        padding[:results.shape[0],:results.shape[1]]=results\n        results=padding\n    return results\n","metadata":{"execution":{"iopub.status.busy":"2022-06-12T15:08:24.994512Z","iopub.execute_input":"2022-06-12T15:08:24.994959Z","iopub.status.idle":"2022-06-12T15:08:25.005418Z","shell.execute_reply.started":"2022-06-12T15:08:24.994921Z","shell.execute_reply":"2022-06-12T15:08:25.004701Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"# get all features of all files by using py audio\ndef extract_features(data_path):\n    X, Y = [], []\n    for path, emotion in zip(data_path.Path, data_path.Emotions):\n        feature = extract_short_term_features(path)\n        for element in feature:\n            X.append(element)\n            Y.append(emotion)\n    features = pd.DataFrame(X)\n    features['labels'] = Y\n    features.to_csv('features.csv', index=False)\n    return features","metadata":{"execution":{"iopub.status.busy":"2022-06-12T15:08:25.176511Z","iopub.execute_input":"2022-06-12T15:08:25.176944Z","iopub.status.idle":"2022-06-12T15:08:25.189505Z","shell.execute_reply.started":"2022-06-12T15:08:25.176907Z","shell.execute_reply":"2022-06-12T15:08:25.188385Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"## todo: to different modes: generate data or use csv file\n## To read the data from csv file - py_audio\ndef load_data_and_features():\n    features=pd.read_csv('../input/py-audio-features-and-data/features_py_audio.csv')\n    data_path = pd.read_csv('../input/py-audio-features-and-data/data_path_py_audio.csv')\n    return features, data_path","metadata":{"execution":{"iopub.status.busy":"2022-06-12T15:08:25.380934Z","iopub.execute_input":"2022-06-12T15:08:25.381407Z","iopub.status.idle":"2022-06-12T15:08:25.398097Z","shell.execute_reply.started":"2022-06-12T15:08:25.381368Z","shell.execute_reply":"2022-06-12T15:08:25.389128Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"def prepare_training_data(data_available):\n    if data_available:\n        features, data_path= load_data_and_features()\n    else:\n        data_path = preprocess_ravdess_data()\n        features = extract_features(data_path)\n        \n    X = features.iloc[: ,:-1].values\n    Y = features['labels'].values\n    # As this is a multiclass classification problem onehotencoding our Y.\n    encoder = OneHotEncoder()\n    Y = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()\n    # splitting data\n    x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=0, shuffle=True)\n    x_train.shape, y_train.shape, x_test.shape, y_test.shape\n    # scaling our data with sklearn's Standard scaler\n    scaler = StandardScaler()\n    x_train = scaler.fit_transform(x_train)\n    x_test = scaler.transform(x_test)\n    x_train.shape, y_train.shape, x_test.shape, y_test.shape\n    # making our data compatible to model.\n    x_train = np.expand_dims(x_train, axis=2)\n    x_test = np.expand_dims(x_test, axis=2)\n    x_train.shape, y_train.shape, x_test.shape, y_test.shape\n    return x_train, x_test, y_train, y_test","metadata":{"execution":{"iopub.status.busy":"2022-06-12T15:08:25.557092Z","iopub.execute_input":"2022-06-12T15:08:25.557708Z","iopub.status.idle":"2022-06-12T15:08:25.572807Z","shell.execute_reply.started":"2022-06-12T15:08:25.557667Z","shell.execute_reply":"2022-06-12T15:08:25.571850Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"def create_speech_model(input_shape):\n    inputs = keras.Input(shape=(input_shape,1))\n\n    #Low level features\n    low_level = layers.Bidirectional(LSTM(256, return_sequences=True))(inputs)\n    low_level = layers.Dropout(0.2)(low_level)\n    low_level = layers.Bidirectional(LSTM(256))(low_level)\n    low_level = layers.Dropout(0.2)(low_level)\n    low_level = layers.Dense(8, activation='relu')(low_level)\n    \n    #High level features \n    high_level = layers.Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu')(inputs)\n    high_level = layers.Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu')(high_level)\n    high_level = layers.Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu')(high_level)\n    high_level = layers.Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu')(high_level)\n    high_level = layers.MaxPooling1D(pool_size=2, strides = 2, padding = 'same')(high_level)\n    high_level = layers.Dropout(0.2)(high_level)\n    high_level = layers.Dense(8, activation='relu')(high_level)\n    high_level = layers.Flatten()(high_level)\n\n    #Concatenate low and high level features\n    merge = layers.concatenate([low_level, high_level], axis=1)\n    merge = layers.Flatten()(merge)\n\n    outputs = Dense(8, activation='softmax')(merge)\n\n    model = keras.Model(inputs, outputs)\n    #plot_model(model, to_file='network_image.png')\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-06-12T15:08:25.722543Z","iopub.execute_input":"2022-06-12T15:08:25.723141Z","iopub.status.idle":"2022-06-12T15:08:25.734677Z","shell.execute_reply.started":"2022-06-12T15:08:25.723109Z","shell.execute_reply":"2022-06-12T15:08:25.733891Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"data_available=True\nepochs = 500\nx_train, x_test, y_train, y_test=prepare_training_data(data_available)\nmodel=create_speech_model(x_train.shape[1])\nmodel.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\nmodel.summary()\nrlrp = ReduceLROnPlateau(monitor='loss', factor=0.4, verbose=0, patience=2, min_lr=0.0000001)\nhistory=model.fit(x_train, y_train, batch_size=64, epochs=epochs, validation_data=(x_test, y_test), callbacks=[rlrp])","metadata":{"execution":{"iopub.status.busy":"2022-06-12T15:08:38.714809Z","iopub.execute_input":"2022-06-12T15:08:38.715267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Just some vizualizations - nothing final yet - todo\n\nprint(\"Accuracy of our model on test data : \" , model.evaluate(x_test,y_test)[1]*100 , \"%\")\n\nepochs = [i for i in range(epochs)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\ntest_acc = history.history['val_accuracy']\ntest_loss = history.history['val_loss']\n\nfig.set_size_inches(20,6)\nax[0].plot(epochs , train_loss , label = 'Training Loss')\nax[0].plot(epochs , test_loss , label = 'Testing Loss')\nax[0].set_title('Training & Testing Loss')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\n\nax[1].plot(epochs , train_acc , label = 'Training Accuracy')\nax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\nax[1].set_title('Training & Testing Accuracy')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-11T19:38:20.010066Z","iopub.execute_input":"2022-06-11T19:38:20.010496Z","iopub.status.idle":"2022-06-11T19:38:30.594223Z","shell.execute_reply.started":"2022-06-11T19:38:20.01046Z","shell.execute_reply":"2022-06-11T19:38:30.593449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predicting on test data.\npred_test = model.predict(x_test)\ny_pred = encoder.inverse_transform(pred_test)\n\ny_test = encoder.inverse_transform(y_test)","metadata":{"execution":{"iopub.status.busy":"2022-06-11T19:38:48.570818Z","iopub.execute_input":"2022-06-11T19:38:48.571178Z","iopub.status.idle":"2022-06-11T19:38:55.376239Z","shell.execute_reply.started":"2022-06-11T19:38:48.571145Z","shell.execute_reply":"2022-06-11T19:38:55.375424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\ndf['Predicted Labels'] = y_pred.flatten()\ndf['Actual Labels'] = y_test.flatten()\n\ndf.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-06-11T19:38:55.377868Z","iopub.execute_input":"2022-06-11T19:38:55.378221Z","iopub.status.idle":"2022-06-11T19:38:55.397184Z","shell.execute_reply.started":"2022-06-11T19:38:55.378186Z","shell.execute_reply":"2022-06-11T19:38:55.396066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize = (12, 10))\ncm = pd.DataFrame(cm , index = [i for i in encoder.categories_] , columns = [i for i in encoder.categories_])\nsns.heatmap(cm, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')\nplt.title('Confusion Matrix', size=20)\nplt.xlabel('Predicted Labels', size=14)\nplt.ylabel('Actual Labels', size=14)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-11T19:38:55.398786Z","iopub.execute_input":"2022-06-11T19:38:55.399169Z","iopub.status.idle":"2022-06-11T19:38:55.912051Z","shell.execute_reply.started":"2022-06-11T19:38:55.399133Z","shell.execute_reply":"2022-06-11T19:38:55.911346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-11T19:38:55.913584Z","iopub.execute_input":"2022-06-11T19:38:55.914399Z","iopub.status.idle":"2022-06-11T19:38:56.406959Z","shell.execute_reply.started":"2022-06-11T19:38:55.914345Z","shell.execute_reply":"2022-06-11T19:38:56.406059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}