{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# todo: check all imports --> put everything on top and label\nimport pandas as pd\nimport numpy as np\n\nimport os\nimport sys\nimport csv\n\n# librosa is a Python library for analyzing audio and music. It can be used to extract the data from the audio files we will see it later.\n! apt-get update\n! apt-get install -y libsndfile-dev\nimport librosa\nimport librosa.display\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\n\n# to play the audio files\nfrom IPython.display import Audio\n\nimport tensorflow.keras\n! pip install np_utils\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\nimport warnings\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n\n! pip install pyAudioAnalysis\n! pip install eyed3\n\nfrom keras.layers import Bidirectional\nfrom tensorflow.keras.layers import Dense, LSTM, Flatten, Dropout, BatchNormalization\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n\nfrom pyAudioAnalysis import audioBasicIO\nfrom pyAudioAnalysis import ShortTermFeatures\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-24T11:09:29.098389Z","iopub.execute_input":"2022-06-24T11:09:29.098818Z","iopub.status.idle":"2022-06-24T11:10:21.769942Z","shell.execute_reply.started":"2022-06-24T11:09:29.098782Z","shell.execute_reply":"2022-06-24T11:10:21.768766Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"## todo: reference repository\n\n# Paths for data.\ndef preprocess_ravdess_data():\n    ravdess = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/\"\n    ravdess_directory_list = os.listdir(ravdess)\n\n    file_emotion = []\n    file_path = []\n    for dir in ravdess_directory_list:\n        # as their are 20 different actors in our previous directory we need to extract files for each actor.\n        actor = os.listdir(ravdess + dir)\n        for file in actor:\n            part = file.split('.')[0]\n            part = part.split('-')\n            # third part in each file represents the emotion associated to that file.\n            file_emotion.append(int(part[2]))\n            file_path.append(ravdess + dir + '/' + file)\n\n    # dataframe for emotion of files\n    emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n\n    # dataframe for path of files.\n    path_df = pd.DataFrame(file_path, columns=['Path'])\n    data_path = pd.concat([emotion_df, path_df], axis=1)\n\n    # changing integers to actual emotions.\n    data_path.Emotions.replace({1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}, inplace=True)\n    data_path.to_csv(\"data_path.csv\",index=False)\n    data_path.head()\n    \n    return data_path","metadata":{"execution":{"iopub.status.busy":"2022-06-24T11:10:21.772653Z","iopub.execute_input":"2022-06-24T11:10:21.773015Z","iopub.status.idle":"2022-06-24T11:10:21.782386Z","shell.execute_reply.started":"2022-06-24T11:10:21.772976Z","shell.execute_reply":"2022-06-24T11:10:21.781345Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def extract_short_term_features(path): # with py audioanalysis\n\n    [Fs, data] = audioBasicIO.read_audio_file(path)# read the wav file\n    data = audioBasicIO.stereo_to_mono(data) \n    results, feature_names = ShortTermFeatures.feature_extraction(data, Fs, 0.050*Fs, 0.025*Fs, deltas=False)\n    if results.shape[1]>100:\n        results=results[:,:100]\n    elif results.shape[1]<100:\n        padding = np.zeros((34,100))\n        padding[:results.shape[0],:results.shape[1]]=results\n        results=padding\n    results = np.transpose(results)\n    return results","metadata":{"execution":{"iopub.status.busy":"2022-06-24T11:10:21.798314Z","iopub.execute_input":"2022-06-24T11:10:21.798589Z","iopub.status.idle":"2022-06-24T11:10:21.807459Z","shell.execute_reply.started":"2022-06-24T11:10:21.798565Z","shell.execute_reply":"2022-06-24T11:10:21.806155Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# get all features of all files by using py audio\ndef extract_features(data_path):\n    features, labels = [], []\n    for path, emotion in zip(data_path.Path, data_path.Emotions):\n        feature = extract_short_term_features(path)\n        features.append(feature)\n        labels.append(emotion)\n    np.save('features', features)\n    np.save('labels', labels)\n    \n    return features, labels","metadata":{"execution":{"iopub.status.busy":"2022-06-24T11:10:21.821908Z","iopub.execute_input":"2022-06-24T11:10:21.823085Z","iopub.status.idle":"2022-06-24T11:10:21.830106Z","shell.execute_reply.started":"2022-06-24T11:10:21.823045Z","shell.execute_reply":"2022-06-24T11:10:21.829120Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"## To read the data from database files\n##todo insert path\ndef load_data_and_features():\n    features = np.load('../input/features-labels-datapath/features.npy')\n    labels = np.load('../input/features-labels-datapath/labels.npy')\n    data_path = pd.read_csv('../input/features-labels-datapath/data_path.csv')\n    return features, labels, data_path","metadata":{"execution":{"iopub.status.busy":"2022-06-24T11:10:21.833262Z","iopub.execute_input":"2022-06-24T11:10:21.833549Z","iopub.status.idle":"2022-06-24T11:10:21.842850Z","shell.execute_reply.started":"2022-06-24T11:10:21.833525Z","shell.execute_reply":"2022-06-24T11:10:21.841873Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#from sklearn.feature_selection import VarianceThreshold\n\ndef prepare_training_data(data_available):\n    # load data or generate features out of the RAVDESS database\n    if data_available:\n        features,labels, data_path= load_data_and_features()\n    else:\n        data_path = preprocess_ravdess_data()\n        features, labels = extract_features(data_path)\n    X = np.stack(features)\n    Y = labels\n    #One hot encoding the labels\n    encoder = OneHotEncoder()\n    Y = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()\n    # splitting data\n    x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=0, shuffle=True, test_size=0.1)\n    # scaling data with sklearn's Standard scaler\n    scaler = StandardScaler()\n    x_train = scaler.fit_transform(x_train.reshape(-1, x_train.shape[-1])).reshape(x_train.shape)\n    x_test = scaler.transform(x_test.reshape(-1, x_test.shape[-1])).reshape(x_test.shape)\n \n    return x_train, x_test, y_train, y_test, encoder","metadata":{"execution":{"iopub.status.busy":"2022-06-24T11:34:13.570064Z","iopub.execute_input":"2022-06-24T11:34:13.570757Z","iopub.status.idle":"2022-06-24T11:34:13.580016Z","shell.execute_reply.started":"2022-06-24T11:34:13.570719Z","shell.execute_reply":"2022-06-24T11:34:13.578825Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"def create_original_speech_model(input_shape):\n    inputs = keras.Input(shape=(input_shape))\n\n    #Low level features\n    low_level = layers.Bidirectional(LSTM(256, return_sequences=True))(inputs)\n    low_level = layers.Dropout(0.2)(low_level)\n    low_level = layers.Bidirectional(LSTM(256))(low_level)\n    low_level = layers.Dropout(0.2)(low_level)\n    low_level = layers.Dense(8, activation='relu')(low_level)\n    \n    #High level features \n    high_level = layers.Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu')(inputs)\n    high_level = layers.Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu')(high_level)\n    high_level = layers.Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu')(high_level)\n    high_level = layers.Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu')(high_level)\n    high_level = layers.MaxPooling1D(pool_size=2, strides = 2, padding = 'same')(high_level)\n    high_level = layers.Dropout(0.2)(high_level)\n    high_level = layers.Dense(8, activation='relu')(high_level)\n    high_level = layers.Flatten()(high_level)\n\n    #Concatenate low and high level features\n    merge = layers.concatenate([low_level, high_level], axis=1)\n    merge = layers.Flatten()(merge)\n\n    outputs = Dense(8, activation='softmax')(merge)\n\n    model = keras.Model(inputs, outputs)\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-06-24T11:58:36.230163Z","iopub.execute_input":"2022-06-24T11:58:36.230816Z","iopub.status.idle":"2022-06-24T11:58:36.243550Z","shell.execute_reply.started":"2022-06-24T11:58:36.230780Z","shell.execute_reply":"2022-06-24T11:58:36.242383Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import layers\nfrom tensorflow.keras import regularizers\ndef create_LSTM_speech_model(input_shape):\n    inputs = keras.Input(shape=(input_shape))\n\n    #Low level features\n    low_level = layers.Bidirectional(LSTM(256, return_sequences=True))(inputs)\n    low_level = layers.Dropout(0.2)(low_level)\n    low_level = layers.Bidirectional(LSTM(256))(low_level)\n    low_level = layers.Dropout(0.2)(low_level)\n    outputs = layers.Dense(8, activation='softmax')(low_level)\n\n    model = keras.Model(inputs, outputs)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-06-24T11:30:29.000169Z","iopub.execute_input":"2022-06-24T11:30:29.000621Z","iopub.status.idle":"2022-06-24T11:30:29.011809Z","shell.execute_reply.started":"2022-06-24T11:30:29.000583Z","shell.execute_reply":"2022-06-24T11:30:29.010853Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"def create_CNN_speech_model(input_shape):\n    inputs = keras.Input(shape=(input_shape))\n    \n    #High level features \n    high_level = layers.Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu')(inputs)\n    high_level = layers.Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu')(high_level)\n    high_level = layers.Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu')(high_level)\n    high_level = layers.Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu')(high_level)\n    high_level = layers.MaxPooling1D(pool_size=2, strides = 2, padding = 'same')(high_level)\n    high_level = layers.Dropout(0.2)(high_level)\n    high_level = layers.Flatten()(high_level)\n\n    outputs = layers.Dense(8, activation='softmax')(high_level)\n\n    model = keras.Model(inputs, outputs)\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-06-24T11:30:29.014544Z","iopub.execute_input":"2022-06-24T11:30:29.014991Z","iopub.status.idle":"2022-06-24T11:30:29.024301Z","shell.execute_reply.started":"2022-06-24T11:30:29.014946Z","shell.execute_reply":"2022-06-24T11:30:29.023363Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"def create_test_speech_model(input_shape):\n    inputs = keras.Input(shape=(input_shape))\n\n    #Low level feature\n    low_level = layers.Bidirectional(LSTM(64, return_sequences=True))(inputs)\n    low_level = layers.Dropout(0.2)(low_level)\n    low_level = layers.Bidirectional(LSTM(32))(low_level)\n    low_level = layers.Dropout(0.3)(low_level)\n    low_level = layers.Dense(8, activation='relu')(low_level)\n    \n    #High level features \n    #high_level = layers.Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu')(inputs)\n    #high_level = layers.MaxPooling1D(pool_size=3, strides = 2, padding = 'same')(high_level)\n    #high_level = layers.Dropout(0.2)(high_level)\n    \n    high_level = layers.Conv1D(64, kernel_size=3, strides=1, padding='same', activation='relu')(inputs)\n    high_level = layers.MaxPooling1D(pool_size=3, strides = 2, padding = 'same')(high_level)\n    high_level = layers.Dropout(0.2)(high_level)\n    \n    high_level = layers.Conv1D(64, kernel_size=3, strides=1, padding='same', activation='relu')(high_level)\n    high_level = layers.MaxPooling1D(pool_size=2, strides = 2, padding = 'same')(high_level)\n    high_level = layers.Dropout(0.2)(high_level)\n    \n    high_level = layers.Conv1D(32, kernel_size=3, strides=1, padding='same', activation='relu')(high_level)\n    high_level = layers.MaxPooling1D(pool_size=2, strides = 2, padding = 'same')(high_level)\n    high_level = layers.Dropout(0.2)(high_level)\n    high_level = layers.Dense(8, activation='relu')(high_level)\n    high_level = layers.Flatten()(high_level)\n\n    #Concatenate low and high level features\n    merge = layers.concatenate([low_level, high_level], axis=1)\n    merge = layers.Flatten()(merge)\n\n    outputs = Dense(8, activation='softmax')(merge)\n\n    model = keras.Model(inputs, outputs)\n    return model\n    ","metadata":{"execution":{"iopub.status.busy":"2022-06-24T12:00:00.799314Z","iopub.execute_input":"2022-06-24T12:00:00.799674Z","iopub.status.idle":"2022-06-24T12:00:00.813439Z","shell.execute_reply.started":"2022-06-24T12:00:00.799644Z","shell.execute_reply":"2022-06-24T12:00:00.812408Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"data_available = True\nepochs=100\nx_train, x_test, y_train, y_test, encoder = prepare_training_data(data_available)\nn_classes = len(y_train[0])\nmodel=create_test_speech_model(x_train.shape[1:])\nopt = keras.optimizers.Adam()\nmodel.compile(optimizer = opt , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\nmodel.summary()\nrlrp = ReduceLROnPlateau(monitor='loss', factor=0.5, verbose=1, patience=5, min_lr=0.0000001)\nearlystopping = EarlyStopping(monitor =\"val_accuracy\", mode = 'auto', patience =100, restore_best_weights = True)\nhistory=model.fit(x_train, y_train, batch_size=64,validation_split=0.2, epochs=epochs, callbacks=[rlrp, earlystopping], shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T12:18:15.741371Z","iopub.execute_input":"2022-06-24T12:18:15.741917Z","iopub.status.idle":"2022-06-24T12:19:43.761401Z","shell.execute_reply.started":"2022-06-24T12:18:15.741874Z","shell.execute_reply":"2022-06-24T12:19:43.760342Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"code","source":"## Just some vizualizations - nothing final yet - todo\n\nprint(\"Accuracy of our model on test data : \" , model.evaluate(x_test,y_test)[1]*100 , \"%\")\n#epochs_num = len(history.history['loss'])\nepochs = [i for i in range(len(history.history['loss']))]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\ntest_acc = history.history['val_accuracy']\ntest_loss = history.history['val_loss']\n\nfig.set_size_inches(20,6)\nax[0].plot(epochs , train_loss , label = 'Training Loss')\nax[0].plot(epochs , test_loss , label = 'Validatoin Loss')\nax[0].set_title('Training & Validation Loss')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\n\nax[1].plot(epochs , train_acc , label = 'Training Accuracy')\nax[1].plot(epochs , test_acc , label = 'Validation Accuracy')\nax[1].set_title('Training & Validation Accuracy')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nplt.savefig('plot.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-24T12:19:43.763676Z","iopub.execute_input":"2022-06-24T12:19:43.764140Z","iopub.status.idle":"2022-06-24T12:19:44.437257Z","shell.execute_reply.started":"2022-06-24T12:19:43.764087Z","shell.execute_reply":"2022-06-24T12:19:44.436293Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"code","source":"# predicting on test data.\npred_test = model.predict(x_test)\ny_pred = encoder.inverse_transform(pred_test)\n\ny_test = encoder.inverse_transform(y_test)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T12:19:44.438938Z","iopub.execute_input":"2022-06-24T12:19:44.439622Z","iopub.status.idle":"2022-06-24T12:19:45.581088Z","shell.execute_reply.started":"2022-06-24T12:19:44.439585Z","shell.execute_reply":"2022-06-24T12:19:45.580139Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\ndf['Predicted Labels'] = y_pred.flatten()\ndf['Actual Labels'] = y_test.flatten()\n\ndf.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T12:19:45.583533Z","iopub.execute_input":"2022-06-24T12:19:45.583868Z","iopub.status.idle":"2022-06-24T12:19:45.600076Z","shell.execute_reply.started":"2022-06-24T12:19:45.583832Z","shell.execute_reply":"2022-06-24T12:19:45.599133Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred)\ncmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\nplt.figure(figsize = (12, 10))\ncmn = pd.DataFrame(cmn , index = [i for i in encoder.categories_] , columns = [i for i in encoder.categories_])\nsns.heatmap(cmn, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')\nplt.title('Confusion Matrix', size=20)\nplt.xlabel('Predicted Labels', size=14)\nplt.ylabel('Actual Labels', size=14)\nplt.savefig('confusion.png')\nplt.show()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-06-24T12:19:45.601350Z","iopub.execute_input":"2022-06-24T12:19:45.601791Z","iopub.status.idle":"2022-06-24T12:19:46.346689Z","shell.execute_reply.started":"2022-06-24T12:19:45.601753Z","shell.execute_reply":"2022-06-24T12:19:46.344725Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-24T12:19:46.348346Z","iopub.execute_input":"2022-06-24T12:19:46.349016Z","iopub.status.idle":"2022-06-24T12:19:46.362031Z","shell.execute_reply.started":"2022-06-24T12:19:46.348969Z","shell.execute_reply":"2022-06-24T12:19:46.360735Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}