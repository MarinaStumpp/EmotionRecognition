{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# todo: check all imports --> put everything on top and label\n# Basic imports\nimport pandas as pd\nimport numpy as np\nimport os\nimport sys\nimport csv\n\n# librosa is a Python library for analyzing audio and music. It can be used to extract the data from the audio files we will see it later.\n! apt-get update\n! apt-get install -y libsndfile-dev\nimport librosa\nimport librosa.display\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pickle\n\n\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\n\n# to play the audio files\nfrom IPython.display import Audio\n\n#Tensorflow and keras\nimport tensorflow as tf\nfrom tensorflow import keras\n! pip install np_utils\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization, LSTM, Bidirectional\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras import regularizers\n\nimport warnings\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n\n# Py audio analysis\n! pip install pyAudioAnalysis\n! pip install eyed3\nfrom pyAudioAnalysis import audioBasicIO\nfrom pyAudioAnalysis import ShortTermFeatures\n\n\n# extract audio from mp4\n!pip install moviepy\nfrom moviepy.editor import VideoFileClip\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-09T11:06:09.823010Z","iopub.execute_input":"2022-07-09T11:06:09.823395Z","iopub.status.idle":"2022-07-09T11:06:48.695812Z","shell.execute_reply.started":"2022-07-09T11:06:09.823361Z","shell.execute_reply":"2022-07-09T11:06:48.694444Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"#todo write loop\ndef convert_video_to_audio(video_file, out_ext=\"mp3\"):\n    filename, ext = os.path.splitext(video_file)\n    clip = VideoFileClip(video_file)\n    clip.audio.write_audiofile(f\"{filename}.{out_ext}\")\n    return f\"{filename}.{out_ext}\"","metadata":{"execution":{"iopub.status.busy":"2022-07-09T11:06:48.702231Z","iopub.execute_input":"2022-07-09T11:06:48.704773Z","iopub.status.idle":"2022-07-09T11:06:48.714195Z","shell.execute_reply.started":"2022-07-09T11:06:48.704728Z","shell.execute_reply":"2022-07-09T11:06:48.712996Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"#convert_video_to_audio('../input/ravdess-video-part3-sad-scare/Sad/Sad/01-01-04-01-01-01-01.mp4')","metadata":{"execution":{"iopub.status.busy":"2022-07-09T11:06:48.719493Z","iopub.execute_input":"2022-07-09T11:06:48.721847Z","iopub.status.idle":"2022-07-09T11:06:48.728216Z","shell.execute_reply.started":"2022-07-09T11:06:48.721810Z","shell.execute_reply":"2022-07-09T11:06:48.727104Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"## todo: reference repository\n\n# Paths for data.\ndef preprocess_ravdess_data():\n    \n    ##todo insert mp4 path\n    ravdess = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/\"\n    ravdess_directory_list = os.listdir(ravdess)\n\n    file_emotion = []\n    file_path = []\n    #todo change dir to path when including mp4 conversion\n    for dir in ravdess_directory_list:\n        ## todo insert mp4 conversion\n        #dir = convert_video_to_audio_moviepy('path')\n        actor = os.listdir(ravdess + dir)\n        for file in actor:\n            part = file.split('.')[0]\n            part = part.split('-')\n            # third part in each file represents the emotion associated to that file.\n            file_emotion.append(int(part[2]))\n            file_path.append(ravdess + dir + '/' + file)\n\n    # dataframe for emotion of files\n    emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n\n    # dataframe for path of files.\n    path_df = pd.DataFrame(file_path, columns=['Path'])\n    data_path = pd.concat([emotion_df, path_df], axis=1)\n\n    # changing integers to actual emotions.\n    data_path.Emotions.replace({1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}, inplace=True)\n    data_path.to_csv(\"data_path.csv\",index=False)\n    data_path.head()\n    \n    return data_path","metadata":{"execution":{"iopub.status.busy":"2022-07-09T11:06:48.733826Z","iopub.execute_input":"2022-07-09T11:06:48.737422Z","iopub.status.idle":"2022-07-09T11:06:48.747638Z","shell.execute_reply.started":"2022-07-09T11:06:48.737385Z","shell.execute_reply":"2022-07-09T11:06:48.746432Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"def extract_short_term_features(path): # with py audioanalysis\n\n    [Fs, data] = audioBasicIO.read_audio_file(path)# read the wav file\n    data = audioBasicIO.stereo_to_mono(data) \n    results, feature_names = ShortTermFeatures.feature_extraction(data, Fs, 0.050*Fs, 0.025*Fs, deltas=False)\n    if results.shape[1]>100:\n        results=results[:,:100]\n    elif results.shape[1]<100:\n        padding = np.zeros((34,100))\n        padding[:results.shape[0],:results.shape[1]]=results\n        results=padding\n    results = np.transpose(results)\n    return results","metadata":{"execution":{"iopub.status.busy":"2022-07-09T11:06:48.749359Z","iopub.execute_input":"2022-07-09T11:06:48.749759Z","iopub.status.idle":"2022-07-09T11:06:48.761487Z","shell.execute_reply.started":"2022-07-09T11:06:48.749724Z","shell.execute_reply":"2022-07-09T11:06:48.760561Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"def extract_mfcc_features(path): \n    data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)\n    result = np.array([])\n    results=[]\n\n    # MFCC extraction\n    results = librosa.feature.mfcc(y=data, sr=sample_rate, n_mfcc=13).T\n    \n    if results.shape[0]>100:\n        results=results[:100,:]\n    elif results.shape[0]<100:\n        padding = np.zeros((100,13))\n        padding[:results.shape[0],:results.shape[1]]=results\n        results=padding\n        \n    return results","metadata":{"execution":{"iopub.status.busy":"2022-07-09T11:06:48.763116Z","iopub.execute_input":"2022-07-09T11:06:48.763832Z","iopub.status.idle":"2022-07-09T11:06:48.772760Z","shell.execute_reply.started":"2022-07-09T11:06:48.763796Z","shell.execute_reply":"2022-07-09T11:06:48.771624Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# get all features of all files by using py audio\ndef extract_features(data_path, feature_type):\n    features, labels = [], []\n    for path, emotion in zip(data_path.Path, data_path.Emotions):\n        if feature_type == 'all':\n            #34 features\n            extract_short_term_features(path)\n        elif feature_type == 'mfcc':\n            #mfcc\n            feature=extract_mfcc_features(path)\n        else:\n            print(\"Feature type not available\")\n        features.append(feature)\n        labels.append(emotion)\n    np.save('features', features)\n    np.save('labels', labels)\n    \n    return features, labels","metadata":{"execution":{"iopub.status.busy":"2022-07-09T11:06:48.775225Z","iopub.execute_input":"2022-07-09T11:06:48.775599Z","iopub.status.idle":"2022-07-09T11:06:48.785653Z","shell.execute_reply.started":"2022-07-09T11:06:48.775563Z","shell.execute_reply":"2022-07-09T11:06:48.784702Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"## Read the data from database files\ndef load_data_and_features(feature_type):\n    if feature_type == 'all':\n        #34 features\n        features = np.load('../input/features-labels-datapath/features.npy')\n        labels = np.load('../input/features-labels-datapath/labels.npy')\n    elif feature_type == 'mfcc':\n        #mfcc\n        features = np.load('../input/features-labels-mfcc/features_mfcc.npy')\n        labels = np.load('../input/features-labels-mfcc/labels_mfcc.npy')\n    else:\n            print(\"Feature type not available\")\n    data_path = pd.read_csv('../input/features-labels-datapath/data_path.csv')\n    return features, labels, data_path","metadata":{"execution":{"iopub.status.busy":"2022-07-09T11:06:48.787201Z","iopub.execute_input":"2022-07-09T11:06:48.787877Z","iopub.status.idle":"2022-07-09T11:06:48.795533Z","shell.execute_reply.started":"2022-07-09T11:06:48.787840Z","shell.execute_reply":"2022-07-09T11:06:48.794597Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"def prepare_training_data(data_available, feature_type):\n    # load data or generate features out of the RAVDESS database\n    if data_available:\n        features,labels, data_path= load_data_and_features(feature_type)\n    else:\n        data_path = preprocess_ravdess_data()\n        features, labels = extract_features(data_path, feature_type)\n    X = np.stack(features)\n    Y = labels\n    #One hot encoding the labels\n    encoder = OneHotEncoder()\n    Y = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()\n    # splitting data\n    x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=0, shuffle=True, test_size=0.2)\n    # scaling data with sklearn's Standard scaler\n    scaler = StandardScaler()\n    x_train = scaler.fit_transform(x_train.reshape(-1, x_train.shape[-1])).reshape(x_train.shape)\n    x_test = scaler.transform(x_test.reshape(-1, x_test.shape[-1])).reshape(x_test.shape)\n \n    return x_train, x_test, y_train, y_test, encoder","metadata":{"execution":{"iopub.status.busy":"2022-07-09T11:06:48.797262Z","iopub.execute_input":"2022-07-09T11:06:48.798708Z","iopub.status.idle":"2022-07-09T11:06:48.808683Z","shell.execute_reply.started":"2022-07-09T11:06:48.798669Z","shell.execute_reply":"2022-07-09T11:06:48.807387Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"def create_original_speech_model(input_shape):\n    inputs = keras.Input(shape=(input_shape))\n\n    #Low level features\n    low_level = layers.Bidirectional(LSTM(256, return_sequences=True))(inputs)\n    low_level = layers.Dropout(0.2)(low_level)\n    low_level = layers.Bidirectional(LSTM(256))(low_level)\n    low_level = layers.Dropout(0.2)(low_level)\n    low_level = layers.Dense(8, activation='relu')(low_level)\n    \n    #High level features \n    high_level = layers.Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu')(inputs)\n    high_level = layers.Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu')(high_level)\n    high_level = layers.Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu')(high_level)\n    high_level = layers.Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu')(high_level)\n    high_level = layers.MaxPooling1D(pool_size=2, strides = 2, padding = 'same')(high_level)\n    high_level = layers.Dropout(0.2)(high_level)\n    high_level = layers.Flatten()(high_level)\n    high_level = layers.Dense(8, activation='relu')(high_level)\n    \n    #Concatenate low and high level features\n    merge = layers.concatenate([low_level, high_level], axis=1)\n    merge = layers.Flatten()(merge)\n\n    outputs = Dense(8, activation='softmax')(merge)\n\n    model = keras.Model(inputs, outputs)\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-07-09T11:38:40.156839Z","iopub.execute_input":"2022-07-09T11:38:40.157846Z","iopub.status.idle":"2022-07-09T11:38:40.169686Z","shell.execute_reply.started":"2022-07-09T11:38:40.157794Z","shell.execute_reply":"2022-07-09T11:38:40.168460Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"def create_LSTM_speech_model(input_shape):\n    inputs = keras.Input(shape=(input_shape))\n\n    #Low level features\n    low_level = layers.Bidirectional(LSTM(256, return_sequences=True))(inputs)\n    low_level = layers.Dropout(0.2)(low_level)\n    low_level = layers.Bidirectional(LSTM(256))(low_level)\n    low_level = layers.Dropout(0.2)(low_level)\n    outputs = layers.Dense(8, activation='softmax')(low_level)\n\n    model = keras.Model(inputs, outputs)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-07-09T11:06:48.829241Z","iopub.execute_input":"2022-07-09T11:06:48.829980Z","iopub.status.idle":"2022-07-09T11:06:48.839072Z","shell.execute_reply.started":"2022-07-09T11:06:48.829940Z","shell.execute_reply":"2022-07-09T11:06:48.837995Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"def create_CNN_speech_model(input_shape):\n    inputs = keras.Input(shape=(input_shape))\n    \n    #High level features \n    high_level = layers.Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu')(inputs)\n    high_level = layers.Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu')(high_level)\n    high_level = layers.Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu')(high_level)\n    high_level = layers.Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu')(high_level)\n    high_level = layers.MaxPooling1D(pool_size=2, strides = 2, padding = 'same')(high_level)\n    high_level = layers.Dropout(0.2)(high_level)\n    high_level = layers.Flatten()(high_level)\n\n    outputs = layers.Dense(8, activation='softmax')(high_level)\n\n    model = keras.Model(inputs, outputs)\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-07-09T11:06:48.841789Z","iopub.execute_input":"2022-07-09T11:06:48.842751Z","iopub.status.idle":"2022-07-09T11:06:48.852258Z","shell.execute_reply.started":"2022-07-09T11:06:48.842709Z","shell.execute_reply":"2022-07-09T11:06:48.851335Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"# This model reaches up to 70-75% validation accuracy\ndef create_test_speech_model(input_shape):\n    inputs = keras.Input(shape=(input_shape))\n\n    #Low level feature\n    low_level = layers.Bidirectional(LSTM(64, return_sequences=True))(inputs)\n    low_level = layers.Dropout(0.5)(low_level)\n    low_level = layers.Bidirectional(LSTM(32))(low_level)\n    low_level = layers.Dropout(0.5)(low_level)\n    low_level = layers.Dense(128)(low_level)\n    low_level = layers.Dropout(0.5)(low_level)\n    low_level = layers.Dense(64)(low_level)\n    low_level = layers.Dropout(0.5)(low_level)\n    \n    #High level features \n    high_level = layers.Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu')(inputs)\n    high_level = layers.MaxPooling1D(pool_size=5, strides = 2, padding = 'same')(high_level)\n    high_level = layers.Dropout(0.4)(high_level)\n    \n    high_level = layers.Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu')(high_level)\n    high_level = layers.MaxPooling1D(pool_size=5, strides = 2, padding = 'same')(high_level)\n    high_level = layers.Dropout(0.3)(high_level)\n    \n    high_level = layers.Conv1D(64, kernel_size=3, strides=1, padding='same', activation='relu')(high_level)\n    high_level = layers.MaxPooling1D(pool_size=3, strides = 2, padding = 'same')(high_level)\n    high_level = layers.Dropout(0.3)(high_level)\n    \n    high_level = layers.Conv1D(64, kernel_size=3, strides=1, padding='same', activation='relu')(high_level)\n    high_level = layers.MaxPooling1D(pool_size=3, strides = 2, padding = 'same')(high_level)\n    high_level = layers.Dropout(0.3)(high_level)\n    high_level = layers.Flatten()(high_level)\n\n    #Concatenate low and high level features\n    merge = layers.concatenate([low_level, high_level], axis=1)\n    merge = layers.Flatten()(merge)\n\n    outputs = Dense(8, activation='softmax')(merge)\n\n    model = keras.Model(inputs, outputs)\n    return model\n    ","metadata":{"execution":{"iopub.status.busy":"2022-07-09T11:06:48.853853Z","iopub.execute_input":"2022-07-09T11:06:48.854689Z","iopub.status.idle":"2022-07-09T11:06:48.870274Z","shell.execute_reply.started":"2022-07-09T11:06:48.854653Z","shell.execute_reply":"2022-07-09T11:06:48.869271Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# This model reaches over 80% validation accuracy\ndef create_CNN_test_speech_model(input_shape):\n    inputs = keras.Input(shape=(input_shape))\n    \n    #High level features \n    high_level = layers.Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu')(inputs)\n    high_level = layers.MaxPooling1D(pool_size=5, strides = 2, padding = 'same')(high_level)\n    high_level = layers.Dropout(0.4)(high_level)\n    \n    high_level = layers.Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu')(high_level)\n    high_level = layers.MaxPooling1D(pool_size=5, strides = 2, padding = 'same')(high_level)\n    high_level = layers.Dropout(0.3)(high_level)\n    \n    high_level = layers.Conv1D(64, kernel_size=3, strides=1, padding='same', activation='relu')(high_level)\n    high_level = layers.MaxPooling1D(pool_size=3, strides = 2, padding = 'same')(high_level)\n    high_level = layers.Dropout(0.3)(high_level)\n    \n    high_level = layers.Conv1D(64, kernel_size=3, strides=1, padding='same', activation='relu')(high_level)\n    high_level = layers.MaxPooling1D(pool_size=3, strides = 2, padding = 'same')(high_level)\n    high_level = layers.Dropout(0.3)(high_level)\n    high_level = layers.Flatten()(high_level)\n\n    outputs = layers.Dense(8, activation='softmax')(high_level)\n\n    model = keras.Model(inputs, outputs)\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-07-09T11:06:48.871822Z","iopub.execute_input":"2022-07-09T11:06:48.872453Z","iopub.status.idle":"2022-07-09T11:06:48.884548Z","shell.execute_reply.started":"2022-07-09T11:06:48.872417Z","shell.execute_reply":"2022-07-09T11:06:48.883573Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"# This model reaches up to 75 % validation accuracy\ndef create_LSTM_test_speech_model(input_shape):\n    inputs = keras.Input(shape=(input_shape))\n\n    low_level = layers.Bidirectional(LSTM(128, return_sequences=True))(inputs)\n    low_level = layers.Dropout(0.5)(low_level)\n    low_level = layers.Bidirectional(LSTM(64))(low_level)\n    low_level = layers.Dropout(0.5)(low_level)\n    low_level = layers.Dense(128)(low_level)\n    low_level = layers.Dropout(0.5)(low_level)\n    low_level = layers.Dense(64)(low_level)\n    low_level = layers.Dropout(0.5)(low_level)\n    \n    outputs = layers.Dense(8, activation='softmax')(low_level)\n\n    model = keras.Model(inputs, outputs)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-07-09T11:06:48.886993Z","iopub.execute_input":"2022-07-09T11:06:48.887670Z","iopub.status.idle":"2022-07-09T11:06:48.896946Z","shell.execute_reply.started":"2022-07-09T11:06:48.887599Z","shell.execute_reply":"2022-07-09T11:06:48.895863Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"pickle_path = \"original_50_runs.dat\"\ndata_available = True\nfeature_type='all' #mfcc or all\nepochs=50\nruns = 50\nplotting_data=[]\nfor i in range(runs):\n    x_train, x_test, y_train, y_test, encoder = prepare_training_data(data_available, feature_type)\n    data_available = True\n    n_classes = len(y_train[0])\n    model=create_original_speech_model(x_train.shape[1:])\n    opt = keras.optimizers.Adam()\n    model.compile(optimizer = opt , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n    model.summary()\n    rlrp = ReduceLROnPlateau(monitor='loss', factor=0.5, verbose=1, patience=5, min_lr=0.0000001)\n    #earlystopping = EarlyStopping(monitor =\"val_accuracy\", mode = 'auto', patience =50, restore_best_weights = True)\n    history=model.fit(x_train, y_train, validation_data=(x_test,y_test), batch_size=64, epochs=epochs, callbacks=[rlrp], shuffle=True)\n    plotting_data.append(history.history)\n\nwith open(pickle_path, \"wb\") as f:\n    pickle.dump((plotting_data), f)","metadata":{"execution":{"iopub.status.busy":"2022-07-09T11:38:51.298683Z","iopub.execute_input":"2022-07-09T11:38:51.299102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Just some vizualizations - nothing final yet - todo\n\nprint(\"Accuracy of our model on test data : \" , model.evaluate(x_test,y_test)[1]*100 , \"%\")\nepochs = [i for i in range(len(history.history['loss']))]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\ntest_acc = history.history['val_accuracy']\ntest_loss = history.history['val_loss']\n\nfig.set_size_inches(20,6)\nax[0].plot(epochs , train_loss , label = 'Training Loss')\nax[0].plot(epochs , test_loss , label = 'Validatoin Loss')\nax[0].set_title('Training & Validation Loss')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\n\nax[1].plot(epochs , train_acc , label = 'Training Accuracy')\nax[1].plot(epochs , test_acc , label = 'Validation Accuracy')\nax[1].set_title('Training & Validation Accuracy')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nplt.savefig('plot.png')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-09T11:08:04.154761Z","iopub.status.idle":"2022-07-09T11:08:04.155566Z","shell.execute_reply.started":"2022-07-09T11:08:04.155290Z","shell.execute_reply":"2022-07-09T11:08:04.155317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predicting on test data.\npred_test = model.predict(x_test)\ny_pred = encoder.inverse_transform(pred_test)\n\ny_test = encoder.inverse_transform(y_test)","metadata":{"execution":{"iopub.status.busy":"2022-07-09T11:08:04.156977Z","iopub.status.idle":"2022-07-09T11:08:04.157675Z","shell.execute_reply.started":"2022-07-09T11:08:04.157423Z","shell.execute_reply":"2022-07-09T11:08:04.157449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\ndf['Predicted Labels'] = y_pred.flatten()\ndf['Actual Labels'] = y_test.flatten()\n\ndf.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T16:56:38.079285Z","iopub.execute_input":"2022-06-28T16:56:38.079961Z","iopub.status.idle":"2022-06-28T16:56:38.094019Z","shell.execute_reply.started":"2022-06-28T16:56:38.079924Z","shell.execute_reply":"2022-06-28T16:56:38.092914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred)\ncmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\nplt.figure(figsize = (12, 10))\ncmn = pd.DataFrame(cm , index = [i for i in encoder.categories_] , columns = [i for i in encoder.categories_])\nsns.heatmap(cmn, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')\nplt.title('Confusion Matrix', size=20)\nplt.xlabel('Predicted Labels', size=14)\nplt.ylabel('Actual Labels', size=14)\nplt.savefig('confusion.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-28T16:57:41.717635Z","iopub.execute_input":"2022-06-28T16:57:41.718522Z","iopub.status.idle":"2022-06-28T16:57:42.402463Z","shell.execute_reply.started":"2022-06-28T16:57:41.718451Z","shell.execute_reply":"2022-06-28T16:57:42.401441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred)\ncmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\nplt.figure(figsize = (12, 10))\ncmn = pd.DataFrame(cmn , index = [i for i in encoder.categories_] , columns = [i for i in encoder.categories_])\nsns.heatmap(cmn, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')\nplt.title('Confusion Matrix', size=20)\nplt.xlabel('Predicted Labels', size=14)\nplt.ylabel('Actual Labels', size=14)\nplt.savefig('confusion.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-24T12:19:45.60135Z","iopub.execute_input":"2022-06-24T12:19:45.601791Z","iopub.status.idle":"2022-06-24T12:19:46.346689Z","shell.execute_reply.started":"2022-06-24T12:19:45.601753Z","shell.execute_reply":"2022-06-24T12:19:46.344725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-28T14:18:37.656201Z","iopub.execute_input":"2022-06-28T14:18:37.657119Z","iopub.status.idle":"2022-06-28T14:18:37.671503Z","shell.execute_reply.started":"2022-06-28T14:18:37.657078Z","shell.execute_reply":"2022-06-28T14:18:37.670417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}