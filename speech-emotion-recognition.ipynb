{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# todo: check all imports --> put everything on top and label\n# Basic imports\nimport pandas as pd\nimport numpy as np\nimport os\nimport sys\nimport csv\n\n# librosa is a Python library for analyzing audio and music. It can be used to extract the data from the audio files we will see it later.\n! apt-get update\n! apt-get install -y libsndfile-dev\nimport librosa\nimport librosa.display\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pickle\n\n\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\n\n# to play the audio files\nfrom IPython.display import Audio\n\n#Tensorflow and keras\nimport tensorflow as tf\nfrom tensorflow import keras\n! pip install np_utils\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization, LSTM, Bidirectional\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras import regularizers\n\nimport warnings\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n\n# Py audio analysis\n! pip install pyAudioAnalysis\n! pip install eyed3\nfrom pyAudioAnalysis import audioBasicIO\nfrom pyAudioAnalysis import ShortTermFeatures\n\n\n# extract audio from mp4\n!pip install moviepy\nfrom moviepy.editor import VideoFileClip\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-08T10:12:12.134139Z","iopub.execute_input":"2022-07-08T10:12:12.135129Z","iopub.status.idle":"2022-07-08T10:12:56.807400Z","shell.execute_reply.started":"2022-07-08T10:12:12.135090Z","shell.execute_reply":"2022-07-08T10:12:56.806190Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"#todo write loop\ndef convert_video_to_audio(video_file, out_ext=\"mp3\"):\n    filename, ext = os.path.splitext(video_file)\n    clip = VideoFileClip(video_file)\n    clip.audio.write_audiofile(f\"{filename}.{out_ext}\")\n    return f\"{filename}.{out_ext}\"","metadata":{"execution":{"iopub.status.busy":"2022-07-08T09:56:53.025108Z","iopub.execute_input":"2022-07-08T09:56:53.026223Z","iopub.status.idle":"2022-07-08T09:56:53.034913Z","shell.execute_reply.started":"2022-07-08T09:56:53.026182Z","shell.execute_reply":"2022-07-08T09:56:53.034042Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#convert_video_to_audio('../input/ravdess-video-part3-sad-scare/Sad/Sad/01-01-04-01-01-01-01.mp4')","metadata":{"execution":{"iopub.status.busy":"2022-07-08T09:56:53.036366Z","iopub.execute_input":"2022-07-08T09:56:53.037086Z","iopub.status.idle":"2022-07-08T09:56:53.145874Z","shell.execute_reply.started":"2022-07-08T09:56:53.037045Z","shell.execute_reply":"2022-07-08T09:56:53.144898Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"## todo: reference repository\n\n# Paths for data.\ndef preprocess_ravdess_data():\n    \n    ##todo insert mp4 path\n    ravdess = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/\"\n    ravdess_directory_list = os.listdir(ravdess)\n\n    file_emotion = []\n    file_path = []\n    #todo change dir to path when including mp4 conversion\n    for dir in ravdess_directory_list:\n        ## todo insert mp4 conversion\n        #dir = convert_video_to_audio_moviepy('path')\n        actor = os.listdir(ravdess + dir)\n        for file in actor:\n            part = file.split('.')[0]\n            part = part.split('-')\n            # third part in each file represents the emotion associated to that file.\n            file_emotion.append(int(part[2]))\n            file_path.append(ravdess + dir + '/' + file)\n\n    # dataframe for emotion of files\n    emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n\n    # dataframe for path of files.\n    path_df = pd.DataFrame(file_path, columns=['Path'])\n    data_path = pd.concat([emotion_df, path_df], axis=1)\n\n    # changing integers to actual emotions.\n    data_path.Emotions.replace({1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}, inplace=True)\n    data_path.to_csv(\"data_path.csv\",index=False)\n    data_path.head()\n    \n    return data_path","metadata":{"execution":{"iopub.status.busy":"2022-07-08T09:56:53.149701Z","iopub.execute_input":"2022-07-08T09:56:53.150108Z","iopub.status.idle":"2022-07-08T09:56:53.163010Z","shell.execute_reply.started":"2022-07-08T09:56:53.150068Z","shell.execute_reply":"2022-07-08T09:56:53.161434Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def extract_short_term_features(path): # with py audioanalysis\n\n    [Fs, data] = audioBasicIO.read_audio_file(path)# read the wav file\n    data = audioBasicIO.stereo_to_mono(data) \n    results, feature_names = ShortTermFeatures.feature_extraction(data, Fs, 0.050*Fs, 0.025*Fs, deltas=False)\n    if results.shape[1]>100:\n        results=results[:,:100]\n    elif results.shape[1]<100:\n        padding = np.zeros((34,100))\n        padding[:results.shape[0],:results.shape[1]]=results\n        results=padding\n    results = np.transpose(results)\n    return results","metadata":{"execution":{"iopub.status.busy":"2022-07-08T09:56:53.166274Z","iopub.execute_input":"2022-07-08T09:56:53.168297Z","iopub.status.idle":"2022-07-08T09:56:53.182394Z","shell.execute_reply.started":"2022-07-08T09:56:53.168218Z","shell.execute_reply":"2022-07-08T09:56:53.181465Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def extract_mfcc_features(path): # with py audioanalysis\n    # ZCR\n    data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)\n    result = np.array([])\n    results=[]\n\n    # MFCC extraction\n    results = librosa.feature.mfcc(y=data, sr=sample_rate, n_mfcc=13).T\n    \n    if results.shape[0]>100:\n        results=results[:100,:]\n    elif results.shape[0]<100:\n        padding = np.zeros((100,13))\n        padding[:results.shape[0],:results.shape[1]]=results\n        results=padding\n        \n    return results","metadata":{"execution":{"iopub.status.busy":"2022-07-08T09:56:53.185263Z","iopub.execute_input":"2022-07-08T09:56:53.185646Z","iopub.status.idle":"2022-07-08T09:56:53.193559Z","shell.execute_reply.started":"2022-07-08T09:56:53.185621Z","shell.execute_reply":"2022-07-08T09:56:53.192655Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# get all features of all files by using py audio\ndef extract_features(data_path, feature_type):\n    features, labels = [], []\n    for path, emotion in zip(data_path.Path, data_path.Emotions):\n        if feature_type == 'all':\n            #34 features\n            extract_short_term_features(path)\n        elif feature_type == 'mfcc':\n            #mfcc\n            feature=extract_mfcc_features(path)\n        else:\n            print(\"Feature type not available\")\n        features.append(feature)\n        labels.append(emotion)\n    np.save('features', features)\n    np.save('labels', labels)\n    \n    return features, labels","metadata":{"execution":{"iopub.status.busy":"2022-07-08T09:56:53.195687Z","iopub.execute_input":"2022-07-08T09:56:53.196795Z","iopub.status.idle":"2022-07-08T09:56:53.206394Z","shell.execute_reply.started":"2022-07-08T09:56:53.196767Z","shell.execute_reply":"2022-07-08T09:56:53.205576Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"## Read the data from database files\ndef load_data_and_features(feature_type):\n    if feature_type == 'all':\n        #34 features\n        features = np.load('../input/features-labels-datapath/features.npy')\n        labels = np.load('../input/features-labels-datapath/labels.npy')\n    elif feature_type == 'mfcc':\n        #mfcc\n        features = np.load('../input/features-labels-mfcc/features_mfcc.npy')\n        labels = np.load('../input/features-labels-mfcc/labels_mfcc.npy')\n    else:\n            print(\"Feature type not available\")\n    data_path = pd.read_csv('../input/features-labels-datapath/data_path.csv')\n    return features, labels, data_path","metadata":{"execution":{"iopub.status.busy":"2022-07-08T09:56:53.208059Z","iopub.execute_input":"2022-07-08T09:56:53.209278Z","iopub.status.idle":"2022-07-08T09:56:53.216947Z","shell.execute_reply.started":"2022-07-08T09:56:53.209190Z","shell.execute_reply":"2022-07-08T09:56:53.215978Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def prepare_training_data(data_available, feature_type):\n    # load data or generate features out of the RAVDESS database\n    if data_available:\n        features,labels, data_path= load_data_and_features(feature_type)\n    else:\n        data_path = preprocess_ravdess_data()\n        features, labels = extract_features(data_path, feature_type)\n    X = np.stack(features)\n    Y = labels\n    #One hot encoding the labels\n    encoder = OneHotEncoder()\n    Y = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()\n    # splitting data\n    x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=0, shuffle=True, test_size=0.1)\n    # scaling data with sklearn's Standard scaler\n    scaler = StandardScaler()\n    x_train = scaler.fit_transform(x_train.reshape(-1, x_train.shape[-1])).reshape(x_train.shape)\n    x_test = scaler.transform(x_test.reshape(-1, x_test.shape[-1])).reshape(x_test.shape)\n \n    return x_train, x_test, y_train, y_test, encoder","metadata":{"execution":{"iopub.status.busy":"2022-07-08T09:56:53.218750Z","iopub.execute_input":"2022-07-08T09:56:53.221253Z","iopub.status.idle":"2022-07-08T09:56:53.230909Z","shell.execute_reply.started":"2022-07-08T09:56:53.221224Z","shell.execute_reply":"2022-07-08T09:56:53.230041Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def create_original_speech_model(input_shape):\n    inputs = keras.Input(shape=(input_shape))\n\n    #Low level features\n    low_level = layers.Bidirectional(LSTM(256, return_sequences=True))(inputs)\n    low_level = layers.Dropout(0.2)(low_level)\n    low_level = layers.Bidirectional(LSTM(256))(low_level)\n    low_level = layers.Dropout(0.2)(low_level)\n    low_level = layers.Dense(8, activation='relu')(low_level)\n    \n    #High level features \n    high_level = layers.Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu')(inputs)\n    high_level = layers.Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu')(high_level)\n    high_level = layers.Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu')(high_level)\n    high_level = layers.Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu')(high_level)\n    high_level = layers.MaxPooling1D(pool_size=2, strides = 2, padding = 'same')(high_level)\n    high_level = layers.Dropout(0.2)(high_level)\n    high_level = layers.Dense(8, activation='relu')(high_level)\n    high_level = layers.Flatten()(high_level)\n\n    #Concatenate low and high level features\n    merge = layers.concatenate([low_level, high_level], axis=1)\n    merge = layers.Flatten()(merge)\n\n    outputs = Dense(8, activation='softmax')(merge)\n\n    model = keras.Model(inputs, outputs)\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-07-08T09:56:53.236075Z","iopub.execute_input":"2022-07-08T09:56:53.236844Z","iopub.status.idle":"2022-07-08T09:56:53.258523Z","shell.execute_reply.started":"2022-07-08T09:56:53.236767Z","shell.execute_reply":"2022-07-08T09:56:53.256925Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def create_LSTM_speech_model(input_shape):\n    inputs = keras.Input(shape=(input_shape))\n\n    #Low level features\n    low_level = layers.Bidirectional(LSTM(256, return_sequences=True))(inputs)\n    low_level = layers.Dropout(0.2)(low_level)\n    low_level = layers.Bidirectional(LSTM(256))(low_level)\n    low_level = layers.Dropout(0.2)(low_level)\n    outputs = layers.Dense(8, activation='softmax')(low_level)\n\n    model = keras.Model(inputs, outputs)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-07-08T09:56:53.260393Z","iopub.execute_input":"2022-07-08T09:56:53.261113Z","iopub.status.idle":"2022-07-08T09:56:53.271514Z","shell.execute_reply.started":"2022-07-08T09:56:53.260993Z","shell.execute_reply":"2022-07-08T09:56:53.270462Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def create_CNN_speech_model(input_shape):\n    inputs = keras.Input(shape=(input_shape))\n    \n    #High level features \n    high_level = layers.Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu')(inputs)\n    high_level = layers.Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu')(high_level)\n    high_level = layers.Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu')(high_level)\n    high_level = layers.Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu')(high_level)\n    high_level = layers.MaxPooling1D(pool_size=2, strides = 2, padding = 'same')(high_level)\n    high_level = layers.Dropout(0.2)(high_level)\n    high_level = layers.Flatten()(high_level)\n\n    outputs = layers.Dense(8, activation='softmax')(high_level)\n\n    model = keras.Model(inputs, outputs)\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-07-08T09:56:53.274925Z","iopub.execute_input":"2022-07-08T09:56:53.276009Z","iopub.status.idle":"2022-07-08T09:56:53.286073Z","shell.execute_reply.started":"2022-07-08T09:56:53.275970Z","shell.execute_reply":"2022-07-08T09:56:53.285095Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# This model reaches up to 70-75% validation accuracy\ndef create_test_speech_model(input_shape):\n    inputs = keras.Input(shape=(input_shape))\n\n    #Low level feature\n    low_level = layers.Bidirectional(LSTM(64, return_sequences=True))(inputs)\n    low_level = layers.Dropout(0.5)(low_level)\n    low_level = layers.Bidirectional(LSTM(32))(low_level)\n    low_level = layers.Dropout(0.5)(low_level)\n    low_level = layers.Dense(128)(low_level)\n    low_level = layers.Dropout(0.5)(low_level)\n    low_level = layers.Dense(64)(low_level)\n    low_level = layers.Dropout(0.5)(low_level)\n    \n    #High level features \n    high_level = layers.Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu')(inputs)\n    high_level = layers.MaxPooling1D(pool_size=5, strides = 2, padding = 'same')(high_level)\n    high_level = layers.Dropout(0.4)(high_level)\n    \n    high_level = layers.Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu')(high_level)\n    high_level = layers.MaxPooling1D(pool_size=5, strides = 2, padding = 'same')(high_level)\n    high_level = layers.Dropout(0.3)(high_level)\n    \n    high_level = layers.Conv1D(64, kernel_size=3, strides=1, padding='same', activation='relu')(high_level)\n    high_level = layers.MaxPooling1D(pool_size=3, strides = 2, padding = 'same')(high_level)\n    high_level = layers.Dropout(0.3)(high_level)\n    \n    high_level = layers.Conv1D(64, kernel_size=3, strides=1, padding='same', activation='relu')(high_level)\n    high_level = layers.MaxPooling1D(pool_size=3, strides = 2, padding = 'same')(high_level)\n    high_level = layers.Dropout(0.3)(high_level)\n    high_level = layers.Flatten()(high_level)\n\n    #Concatenate low and high level features\n    merge = layers.concatenate([low_level, high_level], axis=1)\n    merge = layers.Flatten()(merge)\n\n    outputs = Dense(8, activation='softmax')(merge)\n\n    model = keras.Model(inputs, outputs)\n    return model\n    ","metadata":{"execution":{"iopub.status.busy":"2022-07-08T09:56:53.289255Z","iopub.execute_input":"2022-07-08T09:56:53.290498Z","iopub.status.idle":"2022-07-08T09:56:53.307363Z","shell.execute_reply.started":"2022-07-08T09:56:53.290458Z","shell.execute_reply":"2022-07-08T09:56:53.305507Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# This model reaches over 80% validation accuracy\ndef create_CNN_test_speech_model(input_shape):\n    inputs = keras.Input(shape=(input_shape))\n    \n    #High level features \n    high_level = layers.Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu')(inputs)\n    high_level = layers.MaxPooling1D(pool_size=5, strides = 2, padding = 'same')(high_level)\n    high_level = layers.Dropout(0.4)(high_level)\n    \n    high_level = layers.Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu')(high_level)\n    high_level = layers.MaxPooling1D(pool_size=5, strides = 2, padding = 'same')(high_level)\n    high_level = layers.Dropout(0.3)(high_level)\n    \n    high_level = layers.Conv1D(64, kernel_size=3, strides=1, padding='same', activation='relu')(high_level)\n    high_level = layers.MaxPooling1D(pool_size=3, strides = 2, padding = 'same')(high_level)\n    high_level = layers.Dropout(0.3)(high_level)\n    \n    high_level = layers.Conv1D(64, kernel_size=3, strides=1, padding='same', activation='relu')(high_level)\n    high_level = layers.MaxPooling1D(pool_size=3, strides = 2, padding = 'same')(high_level)\n    high_level = layers.Dropout(0.3)(high_level)\n    high_level = layers.Flatten()(high_level)\n\n    outputs = layers.Dense(8, activation='softmax')(high_level)\n\n    model = keras.Model(inputs, outputs)\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-07-08T09:56:53.309639Z","iopub.execute_input":"2022-07-08T09:56:53.310596Z","iopub.status.idle":"2022-07-08T09:56:53.322597Z","shell.execute_reply.started":"2022-07-08T09:56:53.310557Z","shell.execute_reply":"2022-07-08T09:56:53.321641Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# This model reaches up to 75 % validation accuracy\ndef create_LSTM_test_speech_model(input_shape):\n    inputs = keras.Input(shape=(input_shape))\n\n    low_level = layers.Bidirectional(LSTM(128, return_sequences=True))(inputs)\n    low_level = layers.Dropout(0.5)(low_level)\n    low_level = layers.Bidirectional(LSTM(64))(low_level)\n    low_level = layers.Dropout(0.5)(low_level)\n    low_level = layers.Dense(128)(low_level)\n    low_level = layers.Dropout(0.5)(low_level)\n    low_level = layers.Dense(64)(low_level)\n    low_level = layers.Dropout(0.5)(low_level)\n    \n    outputs = layers.Dense(8, activation='softmax')(low_level)\n\n    model = keras.Model(inputs, outputs)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-07-08T09:56:53.326143Z","iopub.execute_input":"2022-07-08T09:56:53.326447Z","iopub.status.idle":"2022-07-08T09:56:53.336403Z","shell.execute_reply.started":"2022-07-08T09:56:53.326410Z","shell.execute_reply":"2022-07-08T09:56:53.335434Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"pickle_path = \"audio.dat\"\ndata_available = True\nfeature_type='mfcc' #mfcc or all\nepochs=100\nruns = 10\nplotting_data=[]\nfor i in range(runs):\n    x_train, x_test, y_train, y_test, encoder = prepare_training_data(data_available, feature_type)\n    n_classes = len(y_train[0])\n    model=create_CNN_test_speech_model(x_train.shape[1:])\n    opt = keras.optimizers.Adam()\n    model.compile(optimizer = opt , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n    model.summary()\n    rlrp = ReduceLROnPlateau(monitor='loss', factor=0.5, verbose=1, patience=5, min_lr=0.0000001)\n    #earlystopping = EarlyStopping(monitor =\"val_accuracy\", mode = 'auto', patience =50, restore_best_weights = True)\n    history=model.fit(x_train, y_train, batch_size=64,validation_split=0.15, epochs=epochs, callbacks=[rlrp], shuffle=True)\n    plotting_data.append(history.history)\n\nwith open(pickle_path, \"wb\") as f:\n    pickle.dump((plotting_data), f)","metadata":{"execution":{"iopub.status.busy":"2022-07-08T10:44:00.459214Z","iopub.execute_input":"2022-07-08T10:44:00.459580Z","iopub.status.idle":"2022-07-08T10:46:55.334842Z","shell.execute_reply.started":"2022-07-08T10:44:00.459549Z","shell.execute_reply":"2022-07-08T10:46:55.333699Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"## Just some vizualizations - nothing final yet - todo\n\nprint(\"Accuracy of our model on test data : \" , model.evaluate(x_test,y_test)[1]*100 , \"%\")\nepochs = [i for i in range(len(history.history['loss']))]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\ntest_acc = history.history['val_accuracy']\ntest_loss = history.history['val_loss']\n\nfig.set_size_inches(20,6)\nax[0].plot(epochs , train_loss , label = 'Training Loss')\nax[0].plot(epochs , test_loss , label = 'Validatoin Loss')\nax[0].set_title('Training & Validation Loss')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\n\nax[1].plot(epochs , train_acc , label = 'Training Accuracy')\nax[1].plot(epochs , test_acc , label = 'Validation Accuracy')\nax[1].set_title('Training & Validation Accuracy')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nplt.savefig('plot.png')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-28T16:56:33.487472Z","iopub.execute_input":"2022-06-28T16:56:33.488359Z","iopub.status.idle":"2022-06-28T16:56:34.001479Z","shell.execute_reply.started":"2022-06-28T16:56:33.488312Z","shell.execute_reply":"2022-06-28T16:56:34.000534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predicting on test data.\npred_test = model.predict(x_test)\ny_pred = encoder.inverse_transform(pred_test)\n\ny_test = encoder.inverse_transform(y_test)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T16:56:37.004171Z","iopub.execute_input":"2022-06-28T16:56:37.004549Z","iopub.status.idle":"2022-06-28T16:56:37.135367Z","shell.execute_reply.started":"2022-06-28T16:56:37.004514Z","shell.execute_reply":"2022-06-28T16:56:37.134344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\ndf['Predicted Labels'] = y_pred.flatten()\ndf['Actual Labels'] = y_test.flatten()\n\ndf.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T16:56:38.079285Z","iopub.execute_input":"2022-06-28T16:56:38.079961Z","iopub.status.idle":"2022-06-28T16:56:38.094019Z","shell.execute_reply.started":"2022-06-28T16:56:38.079924Z","shell.execute_reply":"2022-06-28T16:56:38.092914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred)\ncmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\nplt.figure(figsize = (12, 10))\ncmn = pd.DataFrame(cm , index = [i for i in encoder.categories_] , columns = [i for i in encoder.categories_])\nsns.heatmap(cmn, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')\nplt.title('Confusion Matrix', size=20)\nplt.xlabel('Predicted Labels', size=14)\nplt.ylabel('Actual Labels', size=14)\nplt.savefig('confusion.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-28T16:57:41.717635Z","iopub.execute_input":"2022-06-28T16:57:41.718522Z","iopub.status.idle":"2022-06-28T16:57:42.402463Z","shell.execute_reply.started":"2022-06-28T16:57:41.718451Z","shell.execute_reply":"2022-06-28T16:57:42.401441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred)\ncmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\nplt.figure(figsize = (12, 10))\ncmn = pd.DataFrame(cmn , index = [i for i in encoder.categories_] , columns = [i for i in encoder.categories_])\nsns.heatmap(cmn, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')\nplt.title('Confusion Matrix', size=20)\nplt.xlabel('Predicted Labels', size=14)\nplt.ylabel('Actual Labels', size=14)\nplt.savefig('confusion.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-24T12:19:45.60135Z","iopub.execute_input":"2022-06-24T12:19:45.601791Z","iopub.status.idle":"2022-06-24T12:19:46.346689Z","shell.execute_reply.started":"2022-06-24T12:19:45.601753Z","shell.execute_reply":"2022-06-24T12:19:46.344725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-28T14:18:37.656201Z","iopub.execute_input":"2022-06-28T14:18:37.657119Z","iopub.status.idle":"2022-06-28T14:18:37.671503Z","shell.execute_reply.started":"2022-06-28T14:18:37.657078Z","shell.execute_reply":"2022-06-28T14:18:37.670417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}